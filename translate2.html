<!DOCTYPE html>
<html>
    <head>
        <title>Translate Sign to Speech</title>
        <link rel="stylesheet" href="stylereal.css">
        <link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat&display=swap" rel="stylesheet">

    </head>
    <body>
        
       
        <a id="Back" href="translateoptions.html">Back</a>
        <div id="videoContainer">
            <video autoplay="true" id="videoElement" style="display:none"></video>
            <canvas id="canvas" width="640" height="520"></canvas>
       </div>
       <div id="Waiting">Fancy AI Stuff...(should only take a few seconds)</div>
       

    
    
    
s

        
       

    </body>
    <script src="https://unpkg.com/@tensorflow/tfjs-core@2.1.0/dist/tf-core.js"></script>
<script src="https://unpkg.com/@tensorflow/tfjs-converter@2.1.0/dist/tf-converter.js"></script>

<!-- You must explicitly require a TF.js backend if you're not using the tfs union bundle. -->
<script src="https://unpkg.com/@tensorflow/tfjs-backend-webgl@2.1.0/dist/tf-backend-webgl.js"></script>
<!-- Alternatively you can use the WASM backend: <script src="https://unpkg.com/@tensorflow/tfjs-backend-wasm@2.1.0/dist/tf-backend-wasm.js"></script> -->
 
<script src="https://unpkg.com/@tensorflow-models/handpose@0.0.6/dist/handpose.js"></script>
<script src="script.js"></script> 
<script>
    setTimeout(() => {  console.log("World!"); }, 5000);
</script>


    <!---
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/hand-pose-detection"></script>
    <script type="text/javascript">
        //Selector for your <video> element
        const video = document.querySelector('#myVidPlayer');
    
        //Core
        window.navigator.mediaDevices.getUserMedia({ video: true })
            .then(stream => {
                video.srcObject = stream;
                video.onloadedmetadata = (e) => {
                    video.play();
                };
            })
            .catch( () => {
                alert('You have give browser the permission to run Webcam and mic ;( ');
            });
    
    </script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands"> </script > 
    <script>
        async function thing(){
            const model = handPoseDetection.SupportedModels.MediaPipeHands;
            const detectorConfig = {
              runtime: 'mediapipe', // or 'tfjs'
              modelType: 'full'
            };
            detector = await handPoseDetection.createDetector(model, detectorConfig);
            const v = document.querySelector('#myVidPlayer');
            const hands = await detector.estimateHands(video);
            console.log(hands);
        }
        
    </script>
    <!---
    <script src="https://cdn.jsdelivr.net/npm/handtrackjs/dist/handtrack.min.js"> </script>
    <script type="text/javascript">
        //Selector for your <video> element
        const video = document.querySelector('#myVidPlayer');
    
        //Core
        window.navigator.mediaDevices.getUserMedia({ video: true })
            .then(stream => {
                video.srcObject = stream;
                video.onloadedmetadata = (e) => {
                    video.play();
                };
            })
            .catch( () => {
                alert('You have give browser the permission to run Webcam and mic ;( ');
            });
    
    </script>
    <script>
        function predict(){
            // Notice there is no 'import' statement. 'handTrack' and 'tf' is
        // available on the index-page because of the script tag above.
                    const v = document.querySelector('#myVidPlayer');
                    const canvas = document.createElement("canvas");
            // scale the canvas accordingly
            canvas.width = v.videoWidth;
            canvas.height = v.videoHeight;
            // draw the video at that frame
            canvas.getContext('2d')
            .drawImage(v, 0, 0, canvas.width, canvas.height);
            
            // convert it to a usable data URL
            const dataURL = canvas.toDataURL();


            var img = document.getElementById("img");
            img.src = dataURL;
                    const i = document.getElementById('img'); 
                    const canva = document.getElementById('canvas');
                    const context = canva.getContext('2d');
                    
                    // Load the model.
                    handTrack.load().then(model => {
                    // detect objects in the image.
                    model.detect(i).then(predictions => {
                        console.log('Predictions: ', predictions); 
                        console.log(predictions[0].bbox);
                        
                        
                    });
                    });
                    
        }
        function thing(){
            
        }
        
      </script>
    -->
    
</html>
